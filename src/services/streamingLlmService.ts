/**
 * Streaming LLM Service
 * Processes content chunks sequentially with intelligent rate limiting and model routing
 */

import { ContentChunk, ChunkingResult } from '@/utils/contentChunker';
import { ModelSelection, modelRouter } from '@/utils/modelRouter';
import { rateLimiter, waitFor } from '@/utils/rateLimiter';
import { llmService } from './llmService';
import { RedesignRequest, RedesignResponse } from '@/types';
import { buildRedesignPrompt } from '@/config/prompts';

export interface ChunkResult {
  chunk: ContentChunk;
  result: Partial<RedesignResponse>;
  model: string;
  processingTime: number;
  tokensUsed: number;
  success: boolean;
  error?: string;
}

export interface StreamingProgress {
  currentChunk: number;
  totalChunks: number;
  completedChunks: number;
  failedChunks: number;
  currentModel: string;
  currentAction: string;
  estimatedTimeRemaining: number; // milliseconds
  tokensUsed: {
    gpt5: number;
    gpt4o: number;
    total: number;
  };
  imagePreservation: {
    totalImages: number;
    processedImages: number;
    preservedImages: number;
  };
}

export interface StreamingResult {
  success: boolean;
  result: RedesignResponse;
  chunks: ChunkResult[];
  metrics: {
    totalProcessingTime: number;
    averageChunkTime: number;
    tokensUsed: { gpt5: number; gpt4o: number; total: number };
    imagePreservation: { total: number; preserved: number; rate: number };
    modelUsage: { gpt5Chunks: number; gpt4oChunks: number };
    rateLimitingEvents: number;
  };
  errors: string[];
  warnings: string[];
}

export type ProgressCallback = (progress: StreamingProgress) => void;

/**
 * Streaming service for processing large websites in chunks
 */
export class StreamingLLMService {
  private static instance: StreamingLLMService;
  
  static getInstance(): StreamingLLMService {
    if (!StreamingLLMService.instance) {
      StreamingLLMService.instance = new StreamingLLMService();
    }
    return StreamingLLMService.instance;
  }

  /**
   * Process website redesign using chunked streaming approach
   */
  async processChunkedRedesign(\n    request: RedesignRequest,\n    chunkingResult: ChunkingResult,\n    onProgress?: ProgressCallback\n  ): Promise<StreamingResult> {\n    const startTime = Date.now();\n    const chunks = chunkingResult.chunks;\n    const chunkResults: ChunkResult[] = [];\n    const errors: string[] = [];\n    const warnings: string[] = [];\n    \n    // Route chunks to optimal models\n    const routingMap = modelRouter.routeMultipleChunks(chunks);\n    const routingMetrics = modelRouter.analyzeRoutingMetrics(chunks, routingMap);\n    \n    console.log(`Processing ${chunks.length} chunks with routing:`, {\n      gpt5Chunks: routingMetrics.gpt5Chunks,\n      gpt4oChunks: routingMetrics.gpt4oChunks,\n      criticalImages: routingMetrics.criticalImagesPreserved,\n      totalImages: routingMetrics.totalImagesPreserved\n    });\n    \n    let tokensUsed = { gpt5: 0, gpt4o: 0, total: 0 };\n    let rateLimitingEvents = 0;\n    let totalImages = 0;\n    let preservedImages = 0;\n    \n    // Count total images for progress tracking\n    for (const chunk of chunks) {\n      totalImages += chunk.images.length;\n    }\n    \n    // Process chunks sequentially in priority order\n    for (let i = 0; i < chunks.length; i++) {\n      const chunk = chunks[i];\n      const routing = routingMap.get(chunk);\n      \n      if (!routing) {\n        errors.push(`No routing found for chunk ${chunk.id}`);\n        continue;\n      }\n      \n      // Update progress\n      if (onProgress) {\n        const progress: StreamingProgress = {\n          currentChunk: i + 1,\n          totalChunks: chunks.length,\n          completedChunks: chunkResults.filter(r => r.success).length,\n          failedChunks: chunkResults.filter(r => !r.success).length,\n          currentModel: routing.model,\n          currentAction: `Processing ${chunk.type} section with ${routing.model}`,\n          estimatedTimeRemaining: this.estimateRemainingTime(chunks, i, chunkResults),\n          tokensUsed,\n          imagePreservation: {\n            totalImages,\n            processedImages: preservedImages,\n            preservedImages\n          }\n        };\n        onProgress(progress);\n      }\n      \n      try {\n        // Check rate limits and wait if necessary\n        if (!rateLimiter.canProcess(chunk.estimatedTokens, routing.model)) {\n          const waitTime = rateLimiter.waitTimeForTokens(chunk.estimatedTokens, routing.model);\n          if (waitTime > 0) {\n            console.log(`Rate limited, waiting ${waitTime}ms for ${routing.model}`);\n            rateLimitingEvents++;\n            \n            if (onProgress) {\n              const waitProgress: StreamingProgress = {\n                currentChunk: i + 1,\n                totalChunks: chunks.length,\n                completedChunks: chunkResults.filter(r => r.success).length,\n                failedChunks: chunkResults.filter(r => !r.success).length,\n                currentModel: routing.model,\n                currentAction: `Rate limited, waiting ${Math.ceil(waitTime / 1000)}s...`,\n                estimatedTimeRemaining: waitTime + this.estimateRemainingTime(chunks, i, chunkResults),\n                tokensUsed,\n                imagePreservation: {\n                  totalImages,\n                  processedImages: preservedImages,\n                  preservedImages\n                }\n              };\n              onProgress(waitProgress);\n            }\n            \n            await waitFor(waitTime);\n          }\n        }\n        \n        // Process the chunk\n        const chunkResult = await this.processChunk(chunk, routing, request);\n        chunkResults.push(chunkResult);\n        \n        // Update metrics\n        if (chunkResult.success) {\n          if (routing.model === 'gpt-5') {\n            tokensUsed.gpt5 += chunkResult.tokensUsed;\n          } else {\n            tokensUsed.gpt4o += chunkResult.tokensUsed;\n          }\n          tokensUsed.total += chunkResult.tokensUsed;\n          \n          // Track image preservation\n          preservedImages += chunk.images.length;\n          \n          // Consume tokens from rate limiter\n          rateLimiter.consumeTokens(chunkResult.tokensUsed, routing.model);\n        } else {\n          errors.push(`Failed to process chunk ${chunk.id}: ${chunkResult.error}`);\n        }\n        \n      } catch (error) {\n        const errorMessage = error instanceof Error ? error.message : 'Unknown error';\n        errors.push(`Error processing chunk ${chunk.id}: ${errorMessage}`);\n        \n        chunkResults.push({\n          chunk,\n          result: {},\n          model: routing.model,\n          processingTime: 0,\n          tokensUsed: 0,\n          success: false,\n          error: errorMessage\n        });\n        \n        // Record rate limiting errors\n        if (errorMessage.includes('429') || errorMessage.includes('rate limit')) {\n          rateLimiter.recordRateLimitError(routing.model);\n          rateLimitingEvents++;\n        }\n      }\n      \n      // Small delay between chunks to be respectful\n      if (i < chunks.length - 1) {\n        await waitFor(100);\n      }\n    }\n    \n    // Aggregate results\n    const aggregatedResult = this.aggregateChunkResults(chunkResults, chunkingResult);\n    const totalTime = Date.now() - startTime;\n    \n    // Final progress update\n    if (onProgress) {\n      const finalProgress: StreamingProgress = {\n        currentChunk: chunks.length,\n        totalChunks: chunks.length,\n        completedChunks: chunkResults.filter(r => r.success).length,\n        failedChunks: chunkResults.filter(r => !r.success).length,\n        currentModel: '',\n        currentAction: 'Aggregating results...',\n        estimatedTimeRemaining: 0,\n        tokensUsed,\n        imagePreservation: {\n          totalImages,\n          processedImages: preservedImages,\n          preservedImages\n        }\n      };\n      onProgress(finalProgress);\n    }\n    \n    return {\n      success: chunkResults.some(r => r.success),\n      result: aggregatedResult,\n      chunks: chunkResults,\n      metrics: {\n        totalProcessingTime: totalTime,\n        averageChunkTime: chunkResults.length > 0 ? totalTime / chunkResults.length : 0,\n        tokensUsed,\n        imagePreservation: {\n          total: totalImages,\n          preserved: preservedImages,\n          rate: totalImages > 0 ? preservedImages / totalImages : 1\n        },\n        modelUsage: {\n          gpt5Chunks: chunkResults.filter(r => r.model === 'gpt-5').length,\n          gpt4oChunks: chunkResults.filter(r => r.model !== 'gpt-5').length\n        },\n        rateLimitingEvents\n      },\n      errors,\n      warnings\n    };\n  }\n  \n  /**\n   * Process a single chunk with specified model\n   */\n  private async processChunk(\n    chunk: ContentChunk,\n    routing: ModelSelection,\n    originalRequest: RedesignRequest\n  ): Promise<ChunkResult> {\n    const startTime = Date.now();\n    \n    try {\n      // Build chunk-specific request\n      const chunkRequest: RedesignRequest = {\n        ...originalRequest,\n        originalWebsite: {\n          ...originalRequest.originalWebsite,\n          html: chunk.html,\n          css: chunk.css,\n          javascript: chunk.javascript,\n          images: chunk.images.map(img => img.src)\n        }\n      };\n      \n      // Add chunk context to user instructions\n      const contextualInstructions = `${originalRequest.userInstructions}\n\nCONTEXT: This is a ${chunk.type} section from a larger website. Maintain consistency with the overall design while focusing on this specific section. PRESERVE ALL IMAGES - they are critical to the website's functionality.`;\n      \n      chunkRequest.userInstructions = contextualInstructions;\n      \n      // Process with appropriate model\n      let result: RedesignResponse;\n      if (routing.model === 'gpt-5') {\n        result = await llmService.generateRedesign(chunkRequest);\n      } else {\n        // For GPT-4o, we need to use chat completions directly\n        result = await this.processWithGPT4o(chunkRequest);\n      }\n      \n      const processingTime = Date.now() - startTime;\n      const tokensUsed = routing.estimatedTokens; // Actual usage would be from API response\n      \n      return {\n        chunk,\n        result,\n        model: routing.model,\n        processingTime,\n        tokensUsed,\n        success: true\n      };\n      \n    } catch (error) {\n      const processingTime = Date.now() - startTime;\n      const errorMessage = error instanceof Error ? error.message : 'Unknown error';\n      \n      return {\n        chunk,\n        result: {},\n        model: routing.model,\n        processingTime,\n        tokensUsed: 0,\n        success: false,\n        error: errorMessage\n      };\n    }\n  }\n  \n  /**\n   * Process chunk with GPT-4o (fallback model)\n   */\n  private async processWithGPT4o(request: RedesignRequest): Promise<RedesignResponse> {\n    // Use the existing LLM service but override the model\n    // This is a simplified approach - in practice, you might want a separate service\n    const originalModel = process.env.LLM_MODEL;\n    process.env.LLM_MODEL = 'gpt-4o';\n    \n    try {\n      const result = await llmService.generateRedesign(request);\n      return result;\n    } finally {\n      // Restore original model\n      if (originalModel) {\n        process.env.LLM_MODEL = originalModel;\n      }\n    }\n  }\n  \n  /**\n   * Aggregate chunk results into final redesign\n   */\n  private aggregateChunkResults(\n    chunkResults: ChunkResult[],\n    chunkingResult: ChunkingResult\n  ): RedesignResponse {\n    const successfulResults = chunkResults.filter(r => r.success);\n    \n    if (successfulResults.length === 0) {\n      throw new Error('No chunks were processed successfully');\n    }\n    \n    // Combine HTML from all chunks in order\n    const htmlParts: string[] = [];\n    const cssParts: string[] = [];\n    const jsParts: string[] = [];\n    const improvements: string[] = [];\n    const rationales: string[] = [];\n    \n    // Sort by chunk type priority for proper HTML structure\n    const sortedResults = successfulResults.sort((a, b) => {\n      const typeOrder = ['header', 'nav', 'hero', 'main', 'product', 'gallery', 'aside', 'footer', 'mixed'];\n      return typeOrder.indexOf(a.chunk.type) - typeOrder.indexOf(b.chunk.type);\n    });\n    \n    for (const chunkResult of sortedResults) {\n      if (chunkResult.result.html) {\n        htmlParts.push(chunkResult.result.html);\n      }\n      if (chunkResult.result.css) {\n        cssParts.push(`/* ${chunkResult.chunk.type} section styles */`);\n        cssParts.push(chunkResult.result.css);\n      }\n      if (chunkResult.result.javascript) {\n        jsParts.push(`// ${chunkResult.chunk.type} section scripts`);\n        jsParts.push(chunkResult.result.javascript);\n      }\n      if (chunkResult.result.improvements) {\n        improvements.push(...chunkResult.result.improvements);\n      }\n      if (chunkResult.result.designRationale) {\n        rationales.push(`${chunkResult.chunk.type}: ${chunkResult.result.designRationale}`);\n      }\n    }\n    \n    // Wrap HTML in proper document structure\n    const completeHtml = `<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>Redesigned Website</title>\n</head>\n<body>\n${htmlParts.join('\\n')}\n</body>\n</html>`;\n    \n    const completeCss = cssParts.join('\\n\\n');\n    const completeJs = jsParts.join('\\n\\n');\n    \n    // Aggregate assets\n    const allAssets = {\n      images: successfulResults.flatMap(r => r.result.assets?.images || []),\n      fonts: successfulResults.flatMap(r => r.result.assets?.fonts || [])\n    };\n    \n    return {\n      html: completeHtml,\n      css: completeCss,\n      javascript: completeJs,\n      assets: allAssets,\n      designRationale: `Chunked processing results:\\n${rationales.join('\\n')}\\n\\nThis design was created by processing ${successfulResults.length} content sections individually to preserve all images while respecting rate limits.`,\n      improvements: [\n        ...improvements,\n        `Processed ${successfulResults.length} sections individually`,\n        `Preserved ${chunkingResult.imageAnalysis.count} images through chunked processing`,\n        'Optimized for rate limits while maintaining design coherence'\n      ]\n    };\n  }\n  \n  /**\n   * Estimate remaining processing time\n   */\n  private estimateRemainingTime(\n    chunks: ContentChunk[],\n    currentIndex: number,\n    completedResults: ChunkResult[]\n  ): number {\n    if (completedResults.length === 0) {\n      return chunks.length * 10000; // Rough estimate: 10 seconds per chunk\n    }\n    \n    const avgTimePerChunk = completedResults.reduce((sum, r) => sum + r.processingTime, 0) / completedResults.length;\n    const remainingChunks = chunks.length - currentIndex;\n    \n    return Math.round(avgTimePerChunk * remainingChunks);\n  }\n  \n  /**\n   * Quick validation that chunked approach is worth it\n   */\n  static shouldUseChunkedProcessing(\n    websiteData: { html: string; css: string; javascript: string },\n    estimatedTokens: number\n  ): boolean {\n    // Use chunked processing if:\n    // 1. Content is large (>100KB)\n    // 2. Estimated tokens exceed 25K (safe margin under 30K limit)\n    // 3. Content has many images\n    \n    const totalSize = websiteData.html.length + websiteData.css.length + websiteData.javascript.length;\n    const imageCount = (websiteData.html.match(/<img[^>]*>/g) || []).length;\n    \n    return totalSize > 100000 || estimatedTokens > 25000 || imageCount > 10;\n  }\n}\n\n// Export singleton instance\nexport const streamingLlmService = StreamingLLMService.getInstance();